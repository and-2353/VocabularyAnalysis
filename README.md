# VocabularyAnalysis
高頻度の単語は文書全体の単語の何パーセントを占めているかの分析

## 対象データ
The NeyYork Times から、ランダムで選んだ10記事のhtmlデータを対象
**総単語数:15980**

## 結果
|見出し単語数|出現回数|カバー率|
|--:|--:|--:|
|100|7975|49.906%|
|200|9514|59.537%|
|300|10479|65.576%|
|400|11205|70.119%|
|500|11774|73.680%|
|600|12256|76.696%|
|700|12656|79.199%|
|800|12968|81.151%|
|900|13268|83.029%|
|1000|13547|84.775%|
|1100|13747|86.026%|
|1200|13947|87.278%|
|1300|14147|88.529%|
|1400|14347|89.781%|
|1500|14507|90.782%|
|1600|14607|91.408%|
|1700|14707|92.034%|
|1800|14807|92.660%|
|1900|14907|93.285%|
|2000|15007|93.911%|
|2100|15107|94.537%|
|2200|15207|95.163%|
|2300|15307|95.788%|
|2400|15407|96.414%|
|2500|15507|97.040%|
|2600|15607|97.666%|
|2700|15707|98.292%|
|2800|15807|98.917%|
|2900|15907|99.543%|
|2973|15980|100.000%|

## 今後の改善点
- 別のデータを対象にする
- 単語数増やす
- 80%とか90%を超えるのは何単語目か
- matplotlib でグラフにする
- 日本語と英語のどちらの方が語彙が多いか?

## 修正が必要な点
- (多分見出し単語数が100, 200ではなく101, 201になってる)→本当は修正が必要
- English wikipedia の国データ10ファイル は語彙が似てるのが多そうと考えてデータ取ったまま放置してる
- 同じ記事には同じ単語が多く登場しそうなのでもう少しデータ量増やした方がよさそう（データ量増やしたらもう少し同じ見出し語数でのカバー率は低くなると予想）